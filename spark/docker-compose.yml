version: "3.7"
services:
#  spark-master:
#    image: spark
#    ports:
#    - "8080:8080"
#    - "7077:7077"
#    command: java -cp "/spark-2.4.0-bin-hadoop2.7/conf/:/spark-2.4.0-bin-hadoop2.7/jars/*" org.apache.spark.deploy.master.Master -h master
#    hostname: master
#    volumes:
#      - applications2:/applications
    
#  spark-worker-1:
#    image: spark
#    command: java -cp "/spark-2.4.0-bin-hadoop2.7/conf/:/spark-2.4.0-bin-hadoop2.7/jars/*" org.apache.spark.deploy.worker.Worker spark-master:7077
#    ports:
#    - "8081:8081"
#    volumes:
#      - applications2:/applications

  spark-history-server:
  # docker run --network custom_network -it hadooptest:latest /bin/sh
  # before start: hdfs dfs -mkdir /spark-logs
    image: spark
    command: java -cp "/spark-2.4.0-bin-hadoop2.7/conf/:/spark-2.4.0-bin-hadoop2.7/jars/*" org.apache.spark.deploy.history.HistoryServer
    environment:
    - SPARK_HOME=/spark-2.4.0-bin-hadoop2.7
    - SPARK_CONF_DIR=/spark-2.4.0-bin-hadoop2.7/conf/
    ports:
    - "18080:18080"
    networks:
      - my-proxy-net
    volumes:
      - spark:/spark-2.4.0-bin-hadoop2.7

networks:
  my-proxy-net:
    external:
      name: custom_network

volumes:
  applications2:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /Users/jsicotte/Documents/workspaces/spark/applications
  spark: